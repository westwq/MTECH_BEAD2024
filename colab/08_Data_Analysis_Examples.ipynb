{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVVVM5V7IQNLEpGZ08AWt/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suriarasai/BEAD2024/blob/main/colab/08_Data_Analysis_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleansing and Integration\n",
        "\n",
        "Once the raw data available, we need to process, clean, and transform it into a format that helps with extracting meaningful, actionable business insights. This process of cleaning, processing, and transforming raw data is known as data cleansing and integration.\n",
        "\n",
        "Every data analytics project consists of a few key stages, including data ingestion, data transformation, and loading into a data lakehouse. Only after the data passes through these stages does it become ready for consumption by end users for descriptive and predictive analytics.\n",
        "\n",
        "There are two common industry practices for undertaking this process, widely known as Extract, Transform, Load (ETL) and Extract, Load, Transform (ELT).\n",
        "\n",
        "In general, data cleansing may involve multiple functions for pre-preparing the raw data and such pipelines are composed using spark functions and user defined functions into pipelines and workflows. Some example includes:\n",
        "\n",
        "*   Handling duplicates\n",
        "*   Handling missing observations\n",
        "*   Handling Outliers\n",
        "*   Computing Correlations\n",
        "*   Other meaningful visualization for inetraction between fatures\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ymvx6OLebVKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark and Google Drive Mount\n",
        "Following are the regular spark installation and google drive mount steps"
      ],
      "metadata": {
        "id": "Ro0ZohNxh-_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install pyspark using pip\n",
        "!pip install --ignore-install -q pyspark\n",
        "# install findspark using pip\n",
        "!pip install --ignore-install -q findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15JOIwQXiHjq",
        "outputId": "8bbd84bf-3833-4c2f-b500-6252f67545f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from pyspark import SparkConf,SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "import collections\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Data Analysis\").config('spark.ui.port', '4050').getOrCreate()"
      ],
      "metadata": {
        "id": "fVq_8yAQiRzs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTKhoWMYigyZ",
        "outputId": "1b725d1a-af14-480a-fe79-45fa4fffdc9b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Functions\n",
        "Here is a list of commonly used PySpark SQL functions along with a one-line description for each:\n",
        "\n",
        "abs: Returns the absolute value of the numeric value.\n",
        "\n",
        "* acos: Returns the arccosine of the numeric value.\n",
        "* add_months: Adds a specified number of months to a date.\n",
        "* approx_count_distinct: Returns the approximate number of distinct items in a group.\n",
        "* avg: Computes the average of a group of values.\n",
        "* base64: Encodes a string using Base64 encoding.\n",
        "* bin: Returns the binary representation of a number.\n",
        "* bround: Returns the value of the column rounded to the nearest integer.\n",
        "* cbrt: Computes the cube root of the numeric value.\n",
        "* ceil: Returns the smallest integer greater than or equal to a numeric value.\n",
        "* coalesce: Returns the first non-null value in the list of arguments.\n",
        "* col: Returns a Column based on the given column name.\n",
        "* collect_list: Returns a list of objects with duplicates.\n",
        "* collect_set: Returns a set of objects with duplicate elements eliminated.\n",
        "* concat: Concatenates multiple input columns together into a single string.\n",
        "* concat_ws: Concatenates multiple input columns together into a single string, with a given separator.\n",
        "* conv: Converts a number from one base to another.\n",
        "* corr: Computes the Pearson correlation coefficient between two columns.\n",
        "* cos: Computes the cosine of the numeric value.\n",
        "* count: Returns the number of rows in a group.\n",
        "* countDistinct: Returns the number of distinct items in a group.\n",
        "* covar_pop: Computes the population covariance between two columns.\n",
        "* covar_samp: Computes the sample covariance between two columns.\n",
        "* crc32: Computes a cyclic redundancy check value (CRC32) for a string.\n",
        "* cume_dist: Computes the cumulative distribution of a value in a group of values.\n",
        "* current_date: Returns the current date.\n",
        "* current_timestamp: Returns the current timestamp.\n",
        "* date_add: Adds a specified number of days to a date.\n",
        "* date_format: Converts a date/timestamp/string to a string formatted as specified.\n",
        "* date_sub: Subtracts a specified number of days from a date.\n",
        "* datediff: Returns the number of days between two dates.\n",
        "* dayofmonth: Returns the day of the month of a date.\n",
        "* dayofweek: Returns the day of the week of a date.\n",
        "* dayofyear: Returns the day of the year of a date.\n",
        "* decode: Decodes a Base64 encoded string.\n",
        "* degrees: Converts an angle measured in radians to degrees.\n",
        "* dense_rank: Computes the dense rank of a value in a group of values.\n",
        "* exp: Computes the exponential of the numeric value.\n",
        "* explode: Creates a new row for each element in the given array or map column.\n",
        "* expm1: Computes the exponential of a numeric value minus one.\n",
        "* factorial: Computes the factorial of a numeric value.\n",
        "* filter: Filters rows using the given condition.\n",
        "* first: Returns the first value in a group of values.\n",
        "* flatten: Merges an array of arrays into a single array.\n",
        "* floor: Returns the largest integer less than or equal to a numeric value.\n",
        "* from_unixtime: Converts a UNIX timestamp to a string formatted as specified.\n",
        "* from_utc_timestamp: Converts a timestamp from UTC to the specified time zone.\n",
        "* greatest: Returns the greatest value of the list of column values.\n",
        "* grouping: Indicates whether a specified column in a GROUP BY list is aggregated or not.\n",
        "* grouping_id: Returns the level of grouping.\n",
        "* hex: Converts a numeric value to a hexadecimal string.\n",
        "* hour: Returns the hour component of a timestamp.\n",
        "* initcap: Capitalizes the first letter of each word in a string.\n",
        "* input_file_name: Returns the name of the file being read.\n",
        "* instr: Returns the position of the first occurrence of a substring in a string.\n",
        "* isnan: Checks if the column contains NaN values.\n",
        "* isnull: Checks if the column contains null values.\n",
        "* json_tuple: Extracts the values associated with the given field names from a JSON string column.\n",
        "* kurtosis: Computes the kurtosis of the numeric column.\n",
        "* lag: Returns the value of a column from a previous row within the window.\n",
        "* last: Returns the last value in a group of values.\n",
        "* last_day: Returns the last day of the month of a date.\n",
        "* lead: Returns the value of a column from the next row within the window.\n",
        "* least: Returns the least value of the list of column values.\n",
        "* length: Returns the length of a string.\n",
        "* levenshtein: Computes the Levenshtein distance between two strings.\n",
        "* lit: Creates a Column of literal value.\n",
        "* locate: Returns the position of the first occurrence of a substring in a string.\n",
        "* log: Computes the logarithm of the numeric value.\n",
        "* log10: Computes the base 10 logarithm of the numeric value.\n",
        "* log1p: Computes the natural logarithm of one plus the numeric value.\n",
        "* log2: Computes the base 2 logarithm of the numeric value.\n",
        "* lower: Converts a string to lowercase.\n",
        "* lpad: Left-pads a string with another string.\n",
        "* ltrim: Trims the spaces from the left end of the string.\n",
        "* max: Returns the maximum value in a group of values.\n",
        "* md5: Computes the MD5 hash of a string.\n",
        "* mean: Computes the mean of a group of values.\n",
        "* min: Returns the minimum value in a group of values.\n",
        "* minute: Returns the minute component of a timestamp.\n",
        "* month: Returns the month component of a date.\n",
        "* months_between: Returns the number of months between two dates.\n",
        "* nanvl: Returns a specified value if the column is NaN.\n",
        "* next_day: Returns the next date of the specified day of the week after a date.\n",
        "* ntile: Divides rows into N buckets.\n",
        "* percent_rank: Computes the relative rank of a value in a group of values.\n",
        "* posexplode: Creates a new row for each element in the given array or map column, with position.\n",
        "* pow: Computes the power of the numeric value.\n",
        "* quarter: Returns the quarter of the year of a date.\n",
        "* radians: Converts an angle measured in degrees to radians.\n",
        "* rand: Generates a random number between 0 and 1.\n",
        "* randn: Generates a random number from the standard normal distribution.\n",
        "* regexp_extract: Extracts a match from a string using a regular expression.\n",
        "* regexp_replace: Replaces a substring in a string using a regular expression.\n",
        "* repeat: Repeats a string a specified number of times.\n",
        "* reverse: Reverses the characters in a string.\n",
        "* rint: Returns the integer closest to the numeric value.\n",
        "* round: Rounds a numeric value to the specified number of decimal places.\n",
        "* row_number: Assigns a unique, sequential number to each row within the window.\n",
        "* rpad: Right-pads a string with another string.\n",
        "* rtrim: Trims the spaces from the right end of the string.\n",
        "* second: Returns the second component of a timestamp.\n",
        "* sha1: Computes the SHA-1 hash of a string.\n",
        "* sha2: Computes the SHA-2 hash of a string with the specified bit length.\n",
        "* shiftLeft: Bitwise left shift.\n",
        "* shiftRight: Bitwise right shift.\n",
        "* shiftRightUnsigned: Unsigned bitwise right shift.\n",
        "* signum: Returns the sign of the numeric value.\n",
        "* sin: Computes the sine of the numeric value.\n",
        "* size: Returns the size of an array or map.\n",
        "* skewness: Computes the skewness of the numeric column.\n",
        "* soundex: Returns the Soundex code of a string.\n",
        "* split: Splits a string around matches of the given regular expression.\n",
        "* sqrt: Computes the square root of the numeric value.\n",
        "* stddev: Computes the standard deviation of a group of values.\n",
        "* stddev_pop: Computes the population standard deviation of a group of values.\n",
        "* stddev_samp: Computes the sample standard deviation of a group of values.\n",
        "* struct: Creates a new struct column.\n",
        "* substr: Returns a substring of a string."
      ],
      "metadata": {
        "id": "nl6cH48WonAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions available in PySpark\n",
        "from pyspark.sql import functions\n",
        "# Similar to python, we can use the dir function to view the avaiable functions\n",
        "print(dir(functions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WApudbr2oq5M",
        "outputId": "4686f4cb-6a52-4e59-e51b-f8e87a48940f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Any', 'ArrayType', 'Callable', 'Column', 'DataFrame', 'DataType', 'Dict', 'Iterable', 'JVMView', 'List', 'Optional', 'PandasUDFType', 'PySparkTypeError', 'PySparkValueError', 'SparkContext', 'StringType', 'StructType', 'TYPE_CHECKING', 'Tuple', 'Type', 'Union', 'UserDefinedFunction', 'UserDefinedTableFunction', 'ValuesView', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_create_column_from_literal', '_create_lambda', '_create_py_udf', '_create_py_udtf', '_from_numpy_type', '_get_jvm_function', '_get_lambda_parameters', '_invoke_binary_math_function', '_invoke_function', '_invoke_function_over_columns', '_invoke_function_over_seq_of_columns', '_invoke_higher_order_function', '_options_to_str', '_test', '_to_java_column', '_to_seq', '_unresolved_named_lambda_variable', 'abs', 'acos', 'acosh', 'add_months', 'aes_decrypt', 'aes_encrypt', 'aggregate', 'any_value', 'approxCountDistinct', 'approx_count_distinct', 'approx_percentile', 'array', 'array_agg', 'array_append', 'array_compact', 'array_contains', 'array_distinct', 'array_except', 'array_insert', 'array_intersect', 'array_join', 'array_max', 'array_min', 'array_position', 'array_prepend', 'array_remove', 'array_repeat', 'array_size', 'array_sort', 'array_union', 'arrays_overlap', 'arrays_zip', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'ascii', 'asin', 'asinh', 'assert_true', 'atan', 'atan2', 'atanh', 'avg', 'base64', 'bin', 'bit_and', 'bit_count', 'bit_get', 'bit_length', 'bit_or', 'bit_xor', 'bitmap_bit_position', 'bitmap_bucket_number', 'bitmap_construct_agg', 'bitmap_count', 'bitmap_or_agg', 'bitwiseNOT', 'bitwise_not', 'bool_and', 'bool_or', 'broadcast', 'bround', 'btrim', 'bucket', 'call_function', 'call_udf', 'cardinality', 'cast', 'cbrt', 'ceil', 'ceiling', 'char', 'char_length', 'character_length', 'coalesce', 'col', 'collect_list', 'collect_set', 'column', 'concat', 'concat_ws', 'contains', 'conv', 'convert_timezone', 'corr', 'cos', 'cosh', 'cot', 'count', 'countDistinct', 'count_distinct', 'count_if', 'count_min_sketch', 'covar_pop', 'covar_samp', 'crc32', 'create_map', 'csc', 'cume_dist', 'curdate', 'current_catalog', 'current_database', 'current_date', 'current_schema', 'current_timestamp', 'current_timezone', 'current_user', 'date_add', 'date_diff', 'date_format', 'date_from_unix_date', 'date_part', 'date_sub', 'date_trunc', 'dateadd', 'datediff', 'datepart', 'day', 'dayofmonth', 'dayofweek', 'dayofyear', 'days', 'decimal', 'decode', 'degrees', 'dense_rank', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'e', 'element_at', 'elt', 'encode', 'endswith', 'equal_null', 'every', 'exists', 'exp', 'explode', 'explode_outer', 'expm1', 'expr', 'extract', 'factorial', 'filter', 'find_in_set', 'first', 'first_value', 'flatten', 'floor', 'forall', 'format_number', 'format_string', 'from_csv', 'from_json', 'from_unixtime', 'from_utc_timestamp', 'functools', 'get', 'get_active_spark_context', 'get_json_object', 'getbit', 'greatest', 'grouping', 'grouping_id', 'has_numpy', 'hash', 'hex', 'histogram_numeric', 'hll_sketch_agg', 'hll_sketch_estimate', 'hll_union', 'hll_union_agg', 'hour', 'hours', 'hypot', 'ifnull', 'ilike', 'initcap', 'inline', 'inline_outer', 'input_file_block_length', 'input_file_block_start', 'input_file_name', 'inspect', 'instr', 'isnan', 'isnotnull', 'isnull', 'java_method', 'json_array_length', 'json_object_keys', 'json_tuple', 'kurtosis', 'lag', 'last', 'last_day', 'last_value', 'lcase', 'lead', 'least', 'left', 'length', 'levenshtein', 'like', 'lit', 'ln', 'localtimestamp', 'locate', 'log', 'log10', 'log1p', 'log2', 'lower', 'lpad', 'ltrim', 'make_date', 'make_dt_interval', 'make_interval', 'make_timestamp', 'make_timestamp_ltz', 'make_timestamp_ntz', 'make_ym_interval', 'map_concat', 'map_contains_key', 'map_entries', 'map_filter', 'map_from_arrays', 'map_from_entries', 'map_keys', 'map_values', 'map_zip_with', 'mask', 'max', 'max_by', 'md5', 'mean', 'median', 'min', 'min_by', 'minute', 'mode', 'monotonically_increasing_id', 'month', 'months', 'months_between', 'named_struct', 'nanvl', 'negate', 'negative', 'next_day', 'now', 'np', 'nth_value', 'ntile', 'nullif', 'nvl', 'nvl2', 'octet_length', 'overlay', 'overload', 'pandas_udf', 'parse_url', 'percent_rank', 'percentile', 'percentile_approx', 'pi', 'pmod', 'posexplode', 'posexplode_outer', 'position', 'positive', 'pow', 'power', 'printf', 'product', 'quarter', 'radians', 'raise_error', 'rand', 'randn', 'rank', 'reduce', 'reflect', 'regexp', 'regexp_count', 'regexp_extract', 'regexp_extract_all', 'regexp_instr', 'regexp_like', 'regexp_replace', 'regexp_substr', 'regr_avgx', 'regr_avgy', 'regr_count', 'regr_intercept', 'regr_r2', 'regr_slope', 'regr_sxx', 'regr_sxy', 'regr_syy', 'repeat', 'replace', 'reverse', 'right', 'rint', 'rlike', 'round', 'row_number', 'rpad', 'rtrim', 'schema_of_csv', 'schema_of_json', 'sec', 'second', 'sentences', 'sequence', 'session_window', 'sha', 'sha1', 'sha2', 'shiftLeft', 'shiftRight', 'shiftRightUnsigned', 'shiftleft', 'shiftright', 'shiftrightunsigned', 'shuffle', 'sign', 'signum', 'sin', 'sinh', 'size', 'skewness', 'slice', 'some', 'sort_array', 'soundex', 'spark_partition_id', 'split', 'split_part', 'sqrt', 'stack', 'startswith', 'std', 'stddev', 'stddev_pop', 'stddev_samp', 'str_to_map', 'struct', 'substr', 'substring', 'substring_index', 'sum', 'sumDistinct', 'sum_distinct', 'sys', 'tan', 'tanh', 'timestamp_micros', 'timestamp_millis', 'timestamp_seconds', 'toDegrees', 'toRadians', 'to_binary', 'to_char', 'to_csv', 'to_date', 'to_json', 'to_number', 'to_str', 'to_timestamp', 'to_timestamp_ltz', 'to_timestamp_ntz', 'to_unix_timestamp', 'to_utc_timestamp', 'to_varchar', 'transform', 'transform_keys', 'transform_values', 'translate', 'trim', 'trunc', 'try_add', 'try_aes_decrypt', 'try_avg', 'try_divide', 'try_element_at', 'try_multiply', 'try_remote_functions', 'try_subtract', 'try_sum', 'try_to_binary', 'try_to_number', 'try_to_timestamp', 'typeof', 'ucase', 'udf', 'udtf', 'unbase64', 'unhex', 'unix_date', 'unix_micros', 'unix_millis', 'unix_seconds', 'unix_timestamp', 'unwrap_udt', 'upper', 'url_decode', 'url_encode', 'user', 'var_pop', 'var_samp', 'variance', 'version', 'warnings', 'weekday', 'weekofyear', 'when', 'width_bucket', 'window', 'window_time', 'xpath', 'xpath_boolean', 'xpath_double', 'xpath_float', 'xpath_int', 'xpath_long', 'xpath_number', 'xpath_short', 'xpath_string', 'xxhash64', 'year', 'years', 'zip_with']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downloading and preprocessing Data from GIT\n",
        "You can load data directly from github repositories with public access as shown below."
      ],
      "metadata": {
        "id": "DU4Q-0GOeQyT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysSX34mMaVen",
        "outputId": "d8df373d-4549-4bba-f54d-4a994d81406f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-19 19:58:22--  https://github.com/suriarasai/BEAD2024/raw/main/data/cleanse/employees.csv\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/suriarasai/BEAD2024/main/data/cleanse/employees.csv [following]\n",
            "--2024-07-19 19:58:22--  https://raw.githubusercontent.com/suriarasai/BEAD2024/main/data/cleanse/employees.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170 [text/plain]\n",
            "Saving to: ‘employees.csv’\n",
            "\n",
            "\remployees.csv         0%[                    ]       0  --.-KB/s               \remployees.csv       100%[===================>]     170  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-19 19:58:22 (4.15 MB/s) - ‘employees.csv’ saved [170/170]\n",
            "\n",
            "drive  employees.csv  sample_data\n"
          ]
        }
      ],
      "source": [
        "!rm employees.csv\n",
        "!wget https://github.com/suriarasai/BEAD2024/raw/main/data/cleanse/employees.csv\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load data from csv to a dataframe.\n",
        "# header=True means PySpark will infer the column names from the first line of the CSV file.\n",
        "# inferSchema=True means PySpark will infer the data types of each column\n",
        "df_pyspark = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
        "df_pyspark.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC4Q5bg1im-n",
        "outputId": "dad7c164-8c15-410f-9757-56491ee5bbcf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+----------+------+\n",
            "|      Name| Age|Experience|Salary|\n",
            "+----------+----+----------+------+\n",
            "|   Dilbert|  23|         1|  5000|\n",
            "|     Alice|  25|         4|  7000|\n",
            "|     Wally|  30|         8|  8000|\n",
            "|Point Head|NULL|         6| 10000|\n",
            "|   Dogbert|  21|      NULL|  6000|\n",
            "|   Catbert|  42|        16| 15000|\n",
            "|   Ratbert|  56|        23| 18000|\n",
            "|      NULL|  26|      NULL|  NULL|\n",
            "|      NULL|NULL|         5| 11000|\n",
            "+----------+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Missing Values\n",
        "PySpark, an interface for Apache Spark in Python, provides robust tools for handling missing values in large datasets, making it invaluable for data preprocessing. With PySpark, users can easily detect and address missing values using built-in functions such as dropna() and fillna(). The dropna() function allows the removal of rows or columns containing null values, while fillna() facilitates filling missing values with specified constants or statistical measures like mean or median. These capabilities ensure data integrity and enable seamless, efficient processing of large-scale data, enhancing the overall data analysis workflow."
      ],
      "metadata": {
        "id": "p14LhnCQjgqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop nan or null value whole row\n",
        "df_pyspark.na.drop().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdAX9u-4jv-S",
        "outputId": "96538eb5-9e6d-4928-e073-bcbb974f4bb5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|Age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|Dilbert| 23|         1|  5000|\n",
            "|  Alice| 25|         4|  7000|\n",
            "|  Wally| 30|         8|  8000|\n",
            "|Catbert| 42|        16| 15000|\n",
            "|Ratbert| 56|        23| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here data drops if in a row we have all the values are null\n",
        "df_pyspark.na.drop(how=\"all\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLISdU-8kB2E",
        "outputId": "9d0c8ca3-2fd4-4ca4-f147-f0519dc588f6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+----------+------+\n",
            "|      Name| Age|Experience|Salary|\n",
            "+----------+----+----------+------+\n",
            "|   Dilbert|  23|         1|  5000|\n",
            "|     Alice|  25|         4|  7000|\n",
            "|     Wally|  30|         8|  8000|\n",
            "|Point Head|NULL|         6| 10000|\n",
            "|   Dogbert|  21|      NULL|  6000|\n",
            "|   Catbert|  42|        16| 15000|\n",
            "|   Ratbert|  56|        23| 18000|\n",
            "|      NULL|  26|      NULL|  NULL|\n",
            "|      NULL|NULL|         5| 11000|\n",
            "+----------+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in order to drop the rows even if one null value it is by default in normal drop\n",
        "df_pyspark.na.drop(how=\"any\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_brx-Kzk_cT",
        "outputId": "088cee0e-3043-4733-8ad3-ec49098dca04"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|Age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|Dilbert| 23|         1|  5000|\n",
            "|  Alice| 25|         4|  7000|\n",
            "|  Wally| 30|         8|  8000|\n",
            "|Catbert| 42|        16| 15000|\n",
            "|Ratbert| 56|        23| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# threshold\n",
        "# to check if there is minimum threshold of specified null values\n",
        "df_pyspark.na.drop(how=\"any\", thresh=2).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70GQ1Gf-lMV6",
        "outputId": "500e265b-3890-4a88-af0a-12ab2ba26e61"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+----------+------+\n",
            "|      Name| Age|Experience|Salary|\n",
            "+----------+----+----------+------+\n",
            "|   Dilbert|  23|         1|  5000|\n",
            "|     Alice|  25|         4|  7000|\n",
            "|     Wally|  30|         8|  8000|\n",
            "|Point Head|NULL|         6| 10000|\n",
            "|   Dogbert|  21|      NULL|  6000|\n",
            "|   Catbert|  42|        16| 15000|\n",
            "|   Ratbert|  56|        23| 18000|\n",
            "|      NULL|NULL|         5| 11000|\n",
            "+----------+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.na.drop(how=\"any\", thresh=3).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWRizqO-lgZQ",
        "outputId": "00610c16-3eb6-438a-dd90-172844bed1f2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+----------+------+\n",
            "|      Name| Age|Experience|Salary|\n",
            "+----------+----+----------+------+\n",
            "|   Dilbert|  23|         1|  5000|\n",
            "|     Alice|  25|         4|  7000|\n",
            "|     Wally|  30|         8|  8000|\n",
            "|Point Head|NULL|         6| 10000|\n",
            "|   Dogbert|  21|      NULL|  6000|\n",
            "|   Catbert|  42|        16| 15000|\n",
            "|   Ratbert|  56|        23| 18000|\n",
            "+----------+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset it will delete null value rows from only selected column\n",
        "df_pyspark.na.drop(how=\"any\", subset=[\"Experience\"]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO-j_lJ2ljHV",
        "outputId": "5ba804b4-0f2b-4681-9ca0-ba462bd60ff0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+----------+------+\n",
            "|      Name| Age|Experience|Salary|\n",
            "+----------+----+----------+------+\n",
            "|   Dilbert|  23|         1|  5000|\n",
            "|     Alice|  25|         4|  7000|\n",
            "|     Wally|  30|         8|  8000|\n",
            "|Point Head|NULL|         6| 10000|\n",
            "|   Catbert|  42|        16| 15000|\n",
            "|   Ratbert|  56|        23| 18000|\n",
            "|      NULL|NULL|         5| 11000|\n",
            "+----------+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset it will delete null value rows from only selected column\n",
        "df_pyspark.na.drop(how=\"any\", subset=[\"Age\"]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2AMNa00lrEx",
        "outputId": "ab2a6a08-9d08-4f5c-b555-321db2d31105"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|Age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|Dilbert| 23|         1|  5000|\n",
            "|  Alice| 25|         4|  7000|\n",
            "|  Wally| 30|         8|  8000|\n",
            "|Dogbert| 21|      NULL|  6000|\n",
            "|Catbert| 42|        16| 15000|\n",
            "|Ratbert| 56|        23| 18000|\n",
            "|   NULL| 26|      NULL|  NULL|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## filling missing value\n",
        "df_pyspark.na.fill(\"Missing Value\").show()\n",
        "# Here the interger value was not filled only the strings were filled\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMOEbYFhlyYx",
        "outputId": "663fc33c-16de-4cd6-b857-5db09cad3ca9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----+----------+------+\n",
            "|         Name| Age|Experience|Salary|\n",
            "+-------------+----+----------+------+\n",
            "|      Dilbert|  23|         1|  5000|\n",
            "|        Alice|  25|         4|  7000|\n",
            "|        Wally|  30|         8|  8000|\n",
            "|   Point Head|NULL|         6| 10000|\n",
            "|      Dogbert|  21|      NULL|  6000|\n",
            "|      Catbert|  42|        16| 15000|\n",
            "|      Ratbert|  56|        23| 18000|\n",
            "|Missing Value|  26|      NULL|  NULL|\n",
            "|Missing Value|NULL|         5| 11000|\n",
            "+-------------+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload Data\n",
        "# In case you also want to treat numberic values this way, do not read the sheet with inferschema\n",
        "df_pyspark = spark.read.csv(\"employees.csv\", header=True, inferSchema=False)\n",
        "df_pyspark.na.fill(\"Missing Value\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLGoRYY9mPDr",
        "outputId": "9aa03ff5-c36f-412a-e25f-1eef21da472b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------------+-------------+-------------+\n",
            "|         Name|          Age|   Experience|       Salary|\n",
            "+-------------+-------------+-------------+-------------+\n",
            "|      Dilbert|           23|            1|         5000|\n",
            "|        Alice|           25|            4|         7000|\n",
            "|        Wally|           30|            8|         8000|\n",
            "|   Point Head|Missing Value|            6|        10000|\n",
            "|      Dogbert|           21|Missing Value|         6000|\n",
            "|      Catbert|           42|           16|        15000|\n",
            "|      Ratbert|           56|           23|        18000|\n",
            "|Missing Value|           26|Missing Value|Missing Value|\n",
            "|Missing Value|Missing Value|            5|        11000|\n",
            "+-------------+-------------+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filing null values by mean you can choose median or mode also but for this infer=True is must to take numeric value as integer and not as a string\n",
        "from pyspark.ml.feature import Imputer\n",
        "df_pyspark = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
        "imputer = Imputer(inputCols=[\"Age\", \"Experience\", \"Salary\"], outputCols=[\"{}_imputed\".format(c) for c in [\"Age\", \"Experience\", \"Salary\"]]).setStrategy(\"mean\")\n",
        "imputer.fit(df_pyspark).transform(df_pyspark).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVFK0UsGmd_F",
        "outputId": "693b6f2a-7353-4c2d-c884-0ab97341d19a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+----------+------+-----------+------------------+--------------+\n",
            "|      Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
            "+----------+----+----------+------+-----------+------------------+--------------+\n",
            "|   Dilbert|  23|         1|  5000|         23|                 1|          5000|\n",
            "|     Alice|  25|         4|  7000|         25|                 4|          7000|\n",
            "|     Wally|  30|         8|  8000|         30|                 8|          8000|\n",
            "|Point Head|NULL|         6| 10000|         31|                 6|         10000|\n",
            "|   Dogbert|  21|      NULL|  6000|         21|                 9|          6000|\n",
            "|   Catbert|  42|        16| 15000|         42|                16|         15000|\n",
            "|   Ratbert|  56|        23| 18000|         56|                23|         18000|\n",
            "|      NULL|  26|      NULL|  NULL|         26|                 9|         10000|\n",
            "|      NULL|NULL|         5| 11000|         31|                 5|         11000|\n",
            "+----------+----+----------+------+-----------+------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Outliers\n",
        "PySpark is highly effective in handling outliers in large datasets, offering tools that streamline data cleaning and preprocessing. Using the DataFrame API, users can easily identify and manage outliers by leveraging statistical functions and custom logic. For example, to detect outliers in a dataset, one can compute the interquartile range (IQR) and then filter out values that fall outside the lower and upper bounds defined by the IQR. Here's a practical example: suppose we have a DataFrame df with a column values. To remove outliers, we calculate the IQR and filter the DataFrame as follows:"
      ],
      "metadata": {
        "id": "StWIbI-fnPwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Sample data creation\n",
        "data = [(1,), (2,), (5,), (7,), (10,), (12,), (20,), (100,)]\n",
        "columns = [\"values\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Etiovl5nlou",
        "outputId": "37c27d4c-d06d-4849-9fd8-0555625c1d9d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|values|\n",
            "+------+\n",
            "|     1|\n",
            "|     2|\n",
            "|     5|\n",
            "|     7|\n",
            "|    10|\n",
            "|    12|\n",
            "|    20|\n",
            "|   100|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, percentile_approx\n",
        "# Calculate the IQR\n",
        "q1, q3 = df.approxQuantile(\"values\", [0.25, 0.75], 0.05)\n",
        "iqr = q3 - q1\n",
        "lower_bound = q1 - 1.5 * iqr\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "# Filter out the outliers\n",
        "df_filtered = df.filter((col(\"values\") >= lower_bound) & (col(\"values\") <= upper_bound))\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrXlgB_pn8DY",
        "outputId": "33d476e8-14ea-4752-ce0c-ff9aae4a0d69"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|values|\n",
            "+------+\n",
            "|     1|\n",
            "|     2|\n",
            "|     5|\n",
            "|     7|\n",
            "|    10|\n",
            "|    12|\n",
            "|    20|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "===Workshop Ends Here=="
      ],
      "metadata": {
        "id": "adNqlL2kqp45"
      }
    }
  ]
}