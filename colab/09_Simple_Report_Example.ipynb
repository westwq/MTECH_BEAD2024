{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYlyKWaNTD7OlZg1pzrzs8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suriarasai/BEAD2024/blob/main/colab/09_Simple_Report_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "In this tutorial, we will learn to use SQLite on the Google Colab notebook.\n",
        "\n",
        "### Spark"
      ],
      "metadata": {
        "id": "RMyz-alNvIj5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s919Wh4sCEp",
        "outputId": "848f18cd-e827-4d3a-d86f-fa8c919cd6cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# install pyspark using pip\n",
        "!pip install --ignore-install -q pyspark\n",
        "# install findspark using pip\n",
        "!pip install --ignore-install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from pyspark import SparkConf,SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "import collections\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Example SQL Use Cases\").config('spark.ui.port', '4050').getOrCreate()"
      ],
      "metadata": {
        "id": "7zmERZrGxp-9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Broadcast Variable\n",
        "A broadcast variable in PySpark is a read-only variable that is shared across all the nodes in a Spark cluster. It allows the programmer to cache a large dataset in the memory of each worker node, so that the data can be accessed efficiently during the computation, instead of being shipped over the network multiple times.\n",
        "\n",
        "1. Reduced Network I/O: Instead of sending the same data repeatedly to each worker node, the data is broadcasted once and stored locally on each node. This reduces the amount of data that needs to be transferred over the network, leading to significant performance gains, especially for large datasets.\n",
        "\n",
        "2. Efficient Data Sharing: Broadcast variables ensure that the data is shared efficiently across all nodes. Each node gets a local copy of the data, which can be accessed with minimal latency, improving the overall speed of the computation.\n",
        "\n",
        "3. Memory Efficiency: By storing the broadcast variable only once per node, memory usage is optimized. Each node holds a single copy of the data, rather than multiple copies, which can be the case if the same data were sent to each task independently.\n",
        "\n",
        "Example 1"
      ],
      "metadata": {
        "id": "gSSQaNRCxQdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample large DataFrame\n",
        "large_df = spark.range(100)\n",
        "large_df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G_-hkmPxwUi",
        "outputId": "6d508303-2375-43f6-c907-bba74b161006"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  5|\n",
            "|  6|\n",
            "|  7|\n",
            "|  8|\n",
            "|  9|\n",
            "+---+\n",
            "only showing top 10 rows\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "large_df.select('id').show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uayZEECtzRMv",
        "outputId": "2b36cb56-eee4-45a8-b3cc-00c30506a58b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  5|\n",
            "|  6|\n",
            "|  7|\n",
            "|  8|\n",
            "|  9|\n",
            "+---+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, udf\n",
        "# Apply filter operation without broadcast variable\n",
        "filtered_data = large_df.filter(\"id > 1\" and col(\"id\") < 4)\n",
        "filtered_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFSNemLS02UQ",
        "outputId": "967dc49e-8157-423c-92ad-a50ffa7c734b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import BooleanType\n",
        "# Broadcast variable example\n",
        "broadcast_var = spark.sparkContext.broadcast([1, 2, 3, 4, 5])\n",
        "\n",
        "# Function to filter data using broadcast variable\n",
        "def filter_data(value):\n",
        "    return value in broadcast_var.value\n",
        "\n",
        "# Register UDF\n",
        "filter_data_udf = udf(filter_data, BooleanType())\n",
        "\n",
        "# Apply filter operation with broadcast variable\n",
        "filtered_data = large_df.filter(filter_data_udf(col(\"id\")))\n",
        "filtered_data.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjuor-llzQd6",
        "outputId": "6e526a76-a5af-40b0-e184-4a36fd6050e6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  5|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 2"
      ],
      "metadata": {
        "id": "-YvrIjCX2n4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"Broadcast Join Example\").getOrCreate()\n",
        "\n",
        "# Sample small DataFrame\n",
        "small_df = spark.createDataFrame([(1, \"A\"), (2, \"B\"), (3, \"C\")], [\"id\", \"value\"])\n",
        "\n",
        "# Sample large DataFrame\n",
        "large_df = spark.range(1000).toDF(\"id\")\n",
        "\n",
        "# Perform broadcast join\n",
        "joined_df = large_df.join(broadcast(small_df), \"id\")\n",
        "\n",
        "# Show the results\n",
        "joined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T72cmsbk2p8Z",
        "outputId": "b8768547-0819-4b9f-d7cb-321d3476c621"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "| id|value|\n",
            "+---+-----+\n",
            "|  1|    A|\n",
            "|  2|    B|\n",
            "|  3|    C|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### coalesce\n",
        "Generally, coalesce is a function that can be used in two different contexts: within SQL expressions to handle null values and as a method to reduce the number of partitions in a DataFrame or RDD.\n",
        "\n",
        "1. SQL Expression: Handling Null Values\n",
        "The coalesce function in SQL is used to return the first non-null value from a list of columns. This is particularly useful when you have multiple columns that might contain null values, and you want to fill these nulls with the values from another column."
      ],
      "metadata": {
        "id": "DKAHFiB-2x48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import coalesce\n",
        "\n",
        "# Sample data\n",
        "data = [(1, None), (None, 2), (None, None), (4, 5)]\n",
        "columns = [\"col1\", \"col2\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Use coalesce to fill null values\n",
        "df.withColumn(\"filled_col\", coalesce(df[\"col1\"], df[\"col2\"])).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZHvclQW29s8",
        "outputId": "f9c10207-6d5a-499d-c432-a53d08be6e67"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----------+\n",
            "|col1|col2|filled_col|\n",
            "+----+----+----------+\n",
            "|   1|NULL|         1|\n",
            "|NULL|   2|         2|\n",
            "|NULL|NULL|      NULL|\n",
            "|   4|   5|         4|\n",
            "+----+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Reducing the Number of Partitions\n",
        "In the context of DataFrames and RDDs, coalesce is used to reduce the number of partitions in a DataFrame or RDD. This is often done to optimize performance when writing to disk or when performing operations that benefit from fewer partitions."
      ],
      "metadata": {
        "id": "kjRoGcx13HDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [(1,), (2,), (3,), (4,)]\n",
        "columns = [\"number\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Repartition DataFrame into 4 partitions\n",
        "df_repartitioned = df.repartition(4)\n",
        "\n",
        "# Reduce the number of partitions to 2\n",
        "df_coalesced = df_repartitioned.coalesce(2)\n",
        "\n",
        "# Show the number of partitions\n",
        "print(\"Number of partitions after coalesce: \", df_coalesced.rdd.getNumPartitions())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJrIGo5g3IFL",
        "outputId": "9e2ad908-673e-460a-935a-69db5dd79f10"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions after coalesce:  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write an SQL query to report how many units in each category have been ordered on each day of the week.\n",
        "\n",
        "Return the result table ordered by category."
      ],
      "metadata": {
        "id": "z7iiCnTo3NRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, dayofweek, sum as _sum, when, coalesce, lit\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Orders and Items DataFrame\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define schema for Orders table\n",
        "orders_schema = StructType([\n",
        "    StructField(\"order_id\", IntegerType(), True),\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"order_date\", StringType(), True),  # Temporarily use StringType\n",
        "    StructField(\"item_id\", IntegerType(), True),\n",
        "    StructField(\"quantity\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Define schema for Items table\n",
        "items_schema = StructType([\n",
        "    StructField(\"item_id\", IntegerType(), True),\n",
        "    StructField(\"item_name\", StringType(), True),\n",
        "    StructField(\"item_category\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Sample data for Orders\n",
        "orders_data = [\n",
        "    (1, 1, \"2024-06-01\", 1, 10),\n",
        "    (2, 1, \"2024-06-08\", 2, 10),\n",
        "    (3, 2, \"2024-06-02\", 1, 5),\n",
        "    (4, 3, \"2024-06-03\", 3, 5),\n",
        "    (5, 4, \"2024-06-04\", 4, 1),\n",
        "    (6, 4, \"2024-06-05\", 5, 5),\n",
        "    (7, 5, \"2024-06-05\", 1, 10),\n",
        "    (8, 5, \"2024-06-14\", 4, 5),\n",
        "    (9, 5, \"2024-06-21\", 3, 5)\n",
        "]\n",
        "\n",
        "# Sample data for Items\n",
        "items_data = [\n",
        "    (1, \"Atomic Habits\", \"Book\"),\n",
        "    (2, \"The little blue book\", \"Book\"),\n",
        "    (3, \"Samsung SmarthPhone\", \"Phone\"),\n",
        "    (4, \"Some Phone 2020\", \"Phone\"),\n",
        "    (5, \"Google Glass\", \"Glasses\"),\n",
        "    (6, \"Random Uniqlo T-Shirt XL\", \"T-Shirt\")\n",
        "]\n",
        "\n",
        "# Create DataFrame for Orders\n",
        "orders_df = spark.createDataFrame(data=orders_data, schema=orders_schema)\n",
        "\n",
        "# Convert order_date from string to date type\n",
        "orders_df = orders_df.withColumn(\"order_date\", orders_df[\"order_date\"].cast(\"date\"))\n",
        "\n",
        "# Create DataFrame for Items\n",
        "items_df = spark.createDataFrame(data=items_data, schema=items_schema)\n",
        "\n",
        "# Join Orders and Items DataFrames\n",
        "joined_df = items_df.join(orders_df, items_df[\"item_id\"] == orders_df[\"item_id\"], \"left\") \\\n",
        "    .groupBy(\"item_category\") \\\n",
        "    .agg(\n",
        "        coalesce(_sum(when(dayofweek(col(\"order_date\")) == 2, col(\"quantity\"))), lit(0)).alias(\"Monday\"),\n",
        "        coalesce(_sum(when(dayofweek(col(\"order_date\")) == 3, col(\"quantity\"))), lit(0)).alias(\"Tuesday\"),\n",
        "        coalesce(_sum(when(dayofweek(col(\"order_date\")) == 4, col(\"quantity\"))), lit(0)).alias(\"Wednesday\"),\n",
        "        coalesce(_sum(when(dayofweek(col(\"order_date\")) == 5, col(\"quantity\"))), lit(0)).alias(\"Thursday\"),\n",
        "        coalesce(_sum(when(dayofweek(col(\"order_date\")) == 6, col(\"quantity\"))), lit(0)).alias(\"Friday\"),\n",
        "        coalesce(_sum(when(dayofweek(col(\"order_date\")) == 7, col(\"quantity\"))), lit(0)).alias(\"Saturday\"),\n",
        "        coalesce(_sum(when(dayofweek(col(\"order_date\")) == 1, col(\"quantity\"))), lit(0)).alias(\"Sunday\")\n",
        "    ) \\\n",
        "    .orderBy(\"item_category\")\n",
        "\n",
        "# Show the result DataFrame\n",
        "joined_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFFCPo3l3QZv",
        "outputId": "4e8bfb63-1db8-4829-bcbd-9213088470fe"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------+-------+---------+--------+------+--------+------+\n",
            "|item_category|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|\n",
            "+-------------+------+-------+---------+--------+------+--------+------+\n",
            "|         Book|     0|      0|       10|       0|     0|      20|     5|\n",
            "|      Glasses|     0|      0|        5|       0|     0|       0|     0|\n",
            "|        Phone|     5|      1|        0|       0|    10|       0|     0|\n",
            "|      T-Shirt|     0|      0|        0|       0|     0|       0|     0|\n",
            "+-------------+------+-------+---------+--------+------+--------+------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}